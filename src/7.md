# Why you should read position papers

My favorite papers to read are position papers -- papers that explain a conflict in an area of research and where the author picks a side and defends their position. Position papers are great for sparking discussion: it's especially good to read a paper where you disagree with the author's point of view and use it to reflect upon your own position.

Position papers are also interesting to look at historically: some position papers won, in which they become part of the mainstream and integrated as common sense. Other times, the position paper loses and the other side wins. Either way, I respect authors who are willing to drive their flag into one side or another and defend their position. It takes a lot of gusto, cause if you're right, it looks like you were preaching to the gospel, and if you're wrong, you lose credibility. I'm all about writing down my take for history to make fun of, so I'm going to write about some of my favorite position papers and why you should read them -- hopefully you can think back to a few of your favorite position papers as well.

P.S. Not all of these are papers, but they are on a position and a writing so they count!

## A Critique of Verbal Behavior

- [A Critique of Verbal Behavior](https://chomsky.info/1967____/)

Psychologists have long wondered how language is acquired by humans. It seems to defy all of our traditional understanding -- people who have been isolated at birth long enough seem not to "acquire" language in the complex ways that socialized humans do. However, children who grow up in otherwise linguistically lacking environments can still command language in a complex way, even if other people around them do not. Thus, language acquisition is probably not all acquired by the environment, or determined genetically since both factors intertwine to determine our control of language.

In the first half of the 1900s, Behaviorism became a popular way of explaining cognition scientifically -- compared to previous movements which were untestable, like Psychoanalytics, which focuses on the unconscious self, Behaviorists sought to make more of psychology directly testable. The Behaviorists turned their eye to language acquisition, and a prominent figure at the time, Skinner, wrote the book "Verbal Behavior", and a then young Chomsky wrote a rebuttal to it.

The crux of the rebuttal takes Skinner's point that external stimuli are of "overwhelming importance" to language acquisition, since behaviorists believe that reinforcement is paramount in enforcing behaviors in humans, and counters it. Chomsky explains that the rules of grammar are far too complicated to be learned from simple interactions with people, and thus, children who utter sentences must have some innate knowledge about the rules of grammar and stimulus helps them learn new words and concepts but not the underlying grammar. Chomsky says "This task [language] is accomplished in an astonishingly short time, to a large extent independently of intelligence, and in a comparable way by all children...".

He concludes by saying: "The fact that all normal children acquire essentially comparable grammars of great complexity with remarkable rapidity suggests that human beings are somehow specially designed to do this, with data-handling or 'hypothesis-formulating' ability of unknown character and complexity."

While it's not true that Skinner believed the language acquisition was purely via stimulus (and so Skinner and Chomsky agreed that their must be something natural to language acquisition), this paper was widely influential in psychology and computation, and sparked a whole set of debates on the computability of language and how we acquire language.

## A Theory of Justice

- [A Theory of Justice](https://en.wikipedia.org/wiki/A_Theory_of_Justice)

A Theory of Justice is a political philosophy book by John Rawls from 1971, who famously talks about a thought experiment called "the original position" also referred to as the "veil of ignorance".

Imagine that you're a soul, and some arbiter gives you two choices for the society you could live in. Since you're just a soul (thus you don't know anything about your physical or mental capabilities or the structure of society), what rules should you employ to make society work out for you?

Rawls argues that with this starting point, the only way to create a society is to create one that is "fair to all", since if you choose to privilege an arbitrary group in society, there is a chance you will not end up in that group, and thus, be disenfranchised. So, you'll lean towards a fair society. Thus, Rawls says, the original position informs a set of principles that create a fair society for all, and one that everyone would agree upon.

The principles are:

1. Each person has a set of basic liberties, compatible with the rest of society.
2. Everyone has an equal opportunity to pursue any office or position
3. Socioeconomic inequalities are set to give the greatest benefit to the worst off.

A Theory of Justice does well in explaining a simple thought experiment and using it as the basis for some axioms to build society and then discuss how said society would function, and sparked a lot of discussion in a lot of different fields.

They even made a musical out of it.

## The End of History

- [The End of History](https://www.amazon.com/End-History-Last-Man/dp/0743284550)

The End of History is a book by Francis Fukuyama in 1992, a political scientist who makes a bold claim: with the fall of the Soviet Union, "the end of history" has come, that Western liberal democracy is the final form of human government, and that it will remain that way, forever.

Fukuyama draws upon Hegel and Marx to explain that history is a progression of different eras with different dominant political frameworks: mercantilism, capitalism, communism, liberal democracy all came one after another and coincided with each other.

This is probably the most "position" writing on this list. I chose this book (it's also a paper) because I completely disagree with it -- in 2024, things look a lot less rosy than they did in 1992. Russia has re-emerged as a power, North Korea gained weapons of mass destruction, China is increasing pressure on Taiwan and Hong Kong, and the political situation in the middle east has completely crumbled after the war on terror.

Fukuyama also intertwines western liberal democracy with economics -- he picks liberal democracy for the political side and capitalism for the economic side. While there has been some turn against liberal democracy and towards autarky, a lot of people changed their minds about the efficacy of capitalism after the Great Financial Crash. Capitalism is on top for now and so is liberal democracy, but will it be that way forever, as Fukuyama so bravely declares? I think not.

Even if I'm wrong, forever is a long time so I'll just keep stalling until I'm dead or right.

## Prospect Theory

- [Prospect Theory](http://www.albacharia.ma/xmlui/bitstream/handle/123456789/31987/kahnmtversky.pdf)

Two psychologists write a math paper and win a Nobel prize in economics. Want any more argument to study widely?

This paper written by Tversky and Kahneman starts out by discussing the way most economists thought of utility at the time, expected utility theory, discusses its shortfalls, and then outlines a new theory, prospect theory.

In economics, you choose a game for participants to play, and all actors have certain choices to make, and all choices grant a certain amount of utility that can be reduced to a single number. All agents in this game should make all choices rationally, given the amount of information they have available. Rational in this case means agents should maximize utility. This is of course mathematically optimal, but as the paper points out, we aren't perfectly rational, and so this assumption is flawed.

Prospect theory notes experimentally that we view losses and gains differently, and are not always risk-neutral. If you're given a choice between losing money and gaining money, people dislike losing money a lot more than gaining money. We tend to remember the bad times more than the good times. That makes sense -- we're all attracted like moths to a flame to bad news or gossip. Why shouldn't it be the case for us as economic actors?

This paper was widely influential, and economics in particular has seen a boon in experimental research -- every year we find more cognitive biases in the ways we interact in economic settings, and learn more about human psychology through these findings.

Sometimes challenging the mainstream has good prospects.

## The Unreasonable Effectiveness of Mathematics in the Natural Sciences

- [The Unreasonable Effectiveness of Mathematics in the Natural Sciences](https://www.maths.ed.ac.uk/~v1ranick/papers/wigner.pdf)

In 1960, a mathematician named Eugene Wigner published this paper, where he notes that mathematics is extremely useful in modeling the real world.

In 2024, I think you could change the title of this piece to "The Unreasonable Effectiveness of Mathematics", full stop. Even as a international relations major taking mainly economics and political science classes, I had a lot of math and statistics classes to take. You just can't read papers in those fields without math. Every economics paper has either statistics or math in it. Almost all political science papers require a solid foundation in statistics to even read.

Mathematics has really won out over here, and it took over the natural sciences a long time ago.

It's kind of funny to read this paper with Wigner giving such basic examples like gravity or using linear algebra in physics, when we live in a world now with AI models using linear algebra to create models that can respond in a chatbot like fashion.

## Why Most Published Research Findings are False

- [Why Most Published Research Findings are False](https://www.tandfonline.com/doi/full/10.1080/09332480.2019.1579573)

In 2005, a physician named John Ioannidis wrote this highly talked about paper that uses some back of the envelope numbers to come to the conclusion that at least 50% of papers (in medicine) might be false positives, and thus, false.

Variables in the study that have an effect return significant results, but there is some acceptance of false-positives, determined by the p-value the researcher chooses. Normally, a false-positive rate of 5%, or less than 0.05 is considered acceptable, so there's a 5% chance that something that did not have an effect shows up as having an effect. So the p-value metric is supposed to make the upper bound of false-positives 5%.

Unfortunately, it's not quite so rosy. Since journals tend to only publish significant results (since scientists read papers to learn what has an effect, instead of combing through articles to find out what had no effect). For an example, let's say there is a researcher, A, who finds that some variable B, is significant in some experiment E. They rush to publish a result and are published. Meanwhile, 19 other researches try the same thing and find no effect, which follows since our p-value is 0.05. However, even though they were 19 chances to disprove the efficacy of changing variable B in the experiment, those weren't published, only the false positive.

Whether or not "most" published research findings are false is up for debate, but Ioannidis points out that experiments in medicine use experiments with smaller sizes in clinical trials to find medicine that works for patients. But this is doomed, since the smaller the sample, the less power of the study and the more likely it is to find a false-positive, which would do more harm than good to society at large.

The paper gives 6 corollaries for ways to find research that's more likely to be wrong:

1. The smaller the studies conducted in a scientific field, the less likely findings are to be true.
2. The smaller the effect size, the less likely findings are to be true.
3. The greater the number and lesser the selection of tested relationships in a scientific field, the less likely findings are to be true.
4. The greater the flexibility in designs, definitions, outcomes, and analytical modes, the less likely findings are to be true.
5. The greater the financial and other interests and prejudices in a scientific field, the less likely findings are to be true.
6. The hotter a field, the less likely findings are to be true.

This paper goes against the grain by questioning how useful currently published research is, and gives a reason for why the current replication crisis is ongoing. Researchers pursue fairly complicated models which give them a large degree of freedom to find significant results, regardless of if they're significant.

## C is not a Low-level Language

- [C is not a Low-level Language](https://dl.acm.org/doi/pdf/10.1145/3212477.3212479)

David Chisnall, a computer architect wrote this paper in 2018 to discuss why C is not a low-level language, and all the troubles that causes, motivated by the recent exploitations in instruction level parallelism, spectre and meltdown.

Low level languages map closely to the underlying hardware, in contrast to higher-level languages, which are more abstract and map moreso towards the way a programmer might think about a problem. C is considered a low-level language, which makes it a fit for writing performant systems, like operating systems, compilers, and databases. However, C is a language that was written for the PDP-11, a 16-bit computer from the 1960s. C defines a virtual machine where instructions execute one by one in serial order. This made sense in when C was written, since computers actually executed instructions one by one.

In 2024, that hasn't been true for decades. Compilers are free to reorder code in order to speed up execution, so C code that you write might be executed in a completely different order than the one that you specify. Even worse? Many instructions are executed on different processors at the same time (the author notes that a modern intel processor has ~180 instructions in flight at the same time), and states that this quest for speed without embedding this into the language caused these exploitations and numerous more throughout the years.

As well, memory is no longer sequential (as the C virtual machine believes) and computers have layers of caches, many more registers, especially ones which can process the same data in parallel, and padding is required on data structures to avoid large performance penalties in data fetching. None of these features are built into C, because C predates all of these requirements, and it would be nigh impossible to add these restrictions into C, and thus, we should look towards newer languages to code in, since the future will likely be on more heterogeneous computing (CPUs, GPUs, FPGAs, mobile, cloud, desktop), and the hardware will have lots of extra functionality to tap into that C has a hard time using.

I like this paper because it challenges common gospel and gives reasons for why our mental model of the code we write is so important. Most programmers don't really care about the virtual machine their code targets, but it has a lot of implications in practice. Code reordering might hoist a pointer dereference before a null check, thus crashing a program that should never crash. A small change in some code might cause a program to run 10x slower due to an increase in cache misses. There are many more problems that occur in practice, but neither the programmer nor the compiler writer really have a way around it, since even the processor can reorder instructions due to the need for speed.

We're all doomed.
